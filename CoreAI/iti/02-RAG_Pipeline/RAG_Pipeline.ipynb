{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b0feb8-e84b-4c49-94a3-4cb2c244701f",
   "metadata": {},
   "source": [
    "# RAG Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b405925-cb1d-48ee-af75-9ba55bfbd3b4",
   "metadata": {},
   "source": [
    "Original [Infotrend's CoreAI](https://github.com/Infotrend-Inc/CoreAI) Demo project: https://github.com/Infotrend-Inc/CoreAI-DemoProjects/tree/main/RAG_Pipeline \n",
    "**(Note: modified to use Ollama)**\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** Pipeline using Ragbits, Docling, Chromadb, and Ollama.\n",
    "\n",
    "To find more details about each tool:\n",
    "1. Ragbits: https://ragbits.deepsense.ai/\n",
    "2. Docling: https://docling-project.github.io/docling/ (used by Ragbits)\n",
    "3. Chromadb: https://www.trychroma.com/\n",
    "4. Ollama: https://www.ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d2072-2e03-4e61-aae9-4be3c132132e",
   "metadata": {},
   "source": [
    "# Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268a142-0283-410c-8eac-3f4c3bae91eb",
   "metadata": {},
   "source": [
    "To support features of this notebook with CoreAI, we need to install some libraries that are not pre-installed but are required. \n",
    "\n",
    "**Create and Activate the Virtual Environment:**\n",
    "\n",
    "Open a terminal within the Jupyter notebook (`File -> New -> Terminal`).\n",
    "Navigate to this project's folder; where we want to set up the environment (where this notebook is located) and run:\n",
    "\n",
    "```bash\n",
    "export PROJECT_NAME=\"RAG_Pipeline\"\n",
    "export PIP_CACHE_DIR=`pwd`/.cache/pip\n",
    "mkdir -p $PIP_CACHE_DIR\n",
    "python -m venv --system-site-packages myvenv\n",
    "source myvenv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=${PROJECT_NAME}_myvenv --display-name=\"Python (${PROJECT_NAME}_myvenv)\"\n",
    "echo \"\"; echo \"Before continuing load the created Python kernel: Python (${PROJECT_NAME}_myvenv)\"\n",
    "```\n",
    "\n",
    "This will create a local virtual environment to contain installed files to the mounted `/iti` folder (and not modify the container's files).\n",
    "\n",
    "**Load the Python kernel described above before running the cell below** (it might take a few seconds for the kernel to appear in the list of kernels).\n",
    "\n",
    "Install the required Libraries (from `requirements.txt`).\n",
    "The rest of this notebook relies on the proper kernel to be loaded and environment variables to be set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497e502-9899-4ef3-a243-4912b68f4cf8",
   "metadata": {},
   "source": [
    "Load the Python kernel described above before running the cell below (it might take a few seconds for the kernel to appear in the list of kernels).\n",
    "\n",
    "The following will set the folder location for download so that they are local to the running container, to provide cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45998f56-89aa-4fbd-8d3b-7c46b3202c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = 'False'\n",
    "def set_env_with_cache_dir(env_var_name: str, subdir: str):\n",
    "    base_cache = os.path.join(os.getcwd(), \".cache\")\n",
    "    full_path = os.path.join(base_cache, subdir)\n",
    "    os.environ[env_var_name] = full_path\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "    print(f\"{env_var_name}={full_path}\")\n",
    "\n",
    "set_env_with_cache_dir(\"PIP_CACHE_DIR\", \"pip\")\n",
    "set_env_with_cache_dir(\"HF_HOME\", \"huggingface\")\n",
    "set_env_with_cache_dir(\"EASYOCR_MODULE_PATH\", \"easyocr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdea42e-a622-42d0-a1ab-4bed0a1a062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5bbcd1",
   "metadata": {},
   "source": [
    "Import the used Python pacakges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b18d9-4036-425c-8847-e8c24054868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import chromadb\n",
    "import torch\n",
    "import numpy as np\n",
    "import uuid\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from openai import OpenAI\n",
    "from chromadb.errors import NotFoundError \n",
    "from typing import List, Tuple, Optional\n",
    "from chromadb.api.types import EmbeddingFunction, Documents\n",
    "from chromadb.config import Settings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from ragbits.core.sources.local import LocalFileSource\n",
    "from ragbits.document_search.documents.document import Document, DocumentType\n",
    "from ragbits.document_search.ingestion.parsers.docling import DoclingDocumentParser\n",
    "from ragbits.document_search.documents.element import TextElement, Element\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"DEVICE: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3db108-cd90-44b3-bf57-c68eed42c37f",
   "metadata": {},
   "source": [
    "## Asynchronous Document Parsing using Docling\n",
    "\n",
    "This cell defines functions to:\n",
    "- Determine file types from paths\n",
    "- Parse single/multiple documents concurrently\n",
    "- Extract structured elements using the Docling parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e03bf-4005-4840-8494-7b5fe08df79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse_document(file_path: str):\n",
    "    path = Path(file_path).resolve()\n",
    "    print(f\"\\n Parsing all elements from '{file_path}'\")\n",
    "    doc_type = get_document_type(path)\n",
    "    source = LocalFileSource(path=path)\n",
    "    document = Document( local_path=path, metadata={ \"document_type\": doc_type, \"source\": source })\n",
    "    parser = DoclingDocumentParser(ignore_images=True)\n",
    "    elements = await parser.parse(document)\n",
    "    return elements\n",
    "\n",
    "def get_document_type(path: Path) -> DocumentType:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        return DocumentType.PDF\n",
    "    elif ext == \".png\":\n",
    "        return DocumentType.PNG \n",
    "    elif ext == \".jpg\":\n",
    "        return DocumentType.JPG \n",
    "    elif ext == \".docx\":\n",
    "        return DocumentType.DOCX\n",
    "    elif ext == \".pptx\":\n",
    "        return DocumentType.PPTX\n",
    "    elif ext == \".xlsx\":\n",
    "        return DocumentType.XLSX\n",
    "    elif ext == \".md\":\n",
    "        return DocumentType.MD\n",
    "    elif ext == \".txt\":\n",
    "        return DocumentType.TXT\n",
    "    elif ext == \".html\":\n",
    "        return DocumentType.HTML\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "async def process_files(file_paths: List[str]):\n",
    "    tasks = [parse_document(file_path) for file_path in file_paths]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5f645-17ea-492e-9c48-7b61a9e51d9a",
   "metadata": {},
   "source": [
    "## Building a Semantic Search Index with ChromaDB\n",
    "\n",
    "This cell defines the `ChromaDBIndex` class, which:\n",
    "- Converts `TextElement` objects into vector embeddings using Sentence Transformers\n",
    "- Stores them in a ChromaDB collection\n",
    "- Supports fast semantic search via cosine similarity\n",
    "\n",
    "The helper function `index_all_elements(...)` initializes and populates the index with parsed document elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915a808-9715-4109-8598-840f333a494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDBIndex:\n",
    "\n",
    "    def __init__(self, model_name, collection_name=\"ragbits_docs\",clear_on_init=False):\n",
    "        chromadb_path= os.getcwd() + \"/.cache/chroma\"\n",
    "        # https://docs.trychroma.com/docs/overview/telemetry\n",
    "        self.client = chromadb.PersistentClient(path=chromadb_path, settings=Settings(anonymized_telemetry=False))\n",
    "        self.collection_name = collection_name\n",
    "        if clear_on_init:\n",
    "            try:\n",
    "                self.client.delete_collection(collection_name)\n",
    "            except NotFoundError:\n",
    "                pass\n",
    "    \n",
    "        try:\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "        except chromadb.errors.NotFoundError:\n",
    "            self.collection = self.client.create_collection(name=collection_name)\n",
    "            \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"using the device as {self.device}\" )\n",
    "        self.model.to(self.device) \n",
    "\n",
    "        self._item_to_source = {}\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp( input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def add_text_elements(self, elements: List[TextElement], source_id: str):\n",
    "        ids = [str(uuid.uuid4()) for _ in elements]\n",
    "        texts = [element.content for element in elements]\n",
    "        metadatas = [{\"source\": source_id, \"location\": str(element.location)} for element in elements]\n",
    "        encoded_input = self.tokenizer( texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model( input_ids=encoded_input[\"input_ids\"], attention_mask=encoded_input[\"attention_mask\"])\n",
    "        \n",
    "        embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).cpu().numpy().tolist()\n",
    "    \n",
    "        self.collection.add( embeddings=embeddings, documents=texts, ids=ids, metadatas=metadatas)\n",
    "    \n",
    "        for item_id in ids:\n",
    "            self._item_to_source[item_id] = source_id\n",
    "    \n",
    "        print(f\"Added {len(ids)} elements from '{source_id}'\")\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        encoded_input = self.tokenizer([query], padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model( input_ids=encoded_input[\"input_ids\"], attention_mask=encoded_input[\"attention_mask\"])\n",
    "        \n",
    "        query_embedding = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).cpu().numpy().tolist()[0]\n",
    "        results = self.collection.query( query_embeddings=[query_embedding], n_results=k )\n",
    "        matched_texts = results.get(\"documents\", [[]])[0]\n",
    "        scores = results.get(\"distances\", [[]])[0]\n",
    "        return [(text, float(score)) for text, score in zip(matched_texts, scores)]\n",
    "\n",
    "def index_all_elements(file_paths: List[str], all_elements_list: List[List[Element]], model_name):\n",
    "    index = ChromaDBIndex(model_name,collection_name=\"ragbits_docs\", clear_on_init=True)\n",
    "\n",
    "    for file_path, elements in zip(file_paths, all_elements_list):\n",
    "        source_id = file_path\n",
    "        text_elements = [el for el in elements if isinstance(el, TextElement)]\n",
    "        index.add_text_elements(text_elements, source_id)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7e6ee-f2f3-4e01-b8bd-9a29b21e0bd8",
   "metadata": {},
   "source": [
    "## Generate Answer from Context using LLM\n",
    "\n",
    "This function uses a LLM to generate an answer to a given question using a specific context. If the context is insufficient, the model is instructed to say so. It returns a concise, context-grounded answer using the provided model and API credentials.\n",
    "\n",
    "**Tip**: Feel free to modify the prompt in `generate_answer()` to better suit our needs or task objectives!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dacb4-c3c0-4186-8442-b3944aa142db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama' # required but ignored\n",
    ")\n",
    "\n",
    "def generate_answer(context, question, temperature, max_tokens, model) -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question.\n",
    "If the context does not contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    \n",
    "    response = client.chat.completions.create( model=model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=temperature, max_tokens=max_tokens)\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299f45a-d318-4927-be64-9e32b7d2c7a7",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Select a model from the list displayed above and assign its name to the `model` variable.\n",
    "\n",
    "We can also adjust:\n",
    "\n",
    "- **Temperature:** Controls the randomness of the response.\n",
    "  - Low (e.g., 0.2): More factual, predictable, and focused answers.\n",
    "  - High (e.g., 1.0): More creative, diverse, and sometimes unexpected answers.\n",
    "\n",
    "</n>\n",
    "\n",
    "- **Max Tokens:** Sets the maximum length of the generated response. A token is roughly a word or part of a word. Use a lower number for short answers and a higher number for more detailed responses. The available limit can vary depending on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a30e9-a272-4ef3-bcba-c25ea1516f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.1:8b\"\n",
    "temperature = 0.5\n",
    "max_tokens = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e14825-dcb4-426e-9c6e-6f9d4b45eb12",
   "metadata": {},
   "source": [
    "## Set Embedding Model\n",
    "\n",
    "This model is used to convert document text into vector embeddings for semantic search.\n",
    "Choose any embedding model supported by `sentence-transformers` or **Hugging Face** `transformers`.\n",
    "It is recommended to select a model that fits the performance and accuracy needs.\n",
    "\n",
    "**Note:** Some embedding models require permission to execute custom code during their operation. These models may also come with specific dependencies that are not included by default. Always review the model documentation before allowing code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958da1d-0902-4b4d-996b-9529e6b6932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"mixedbread-ai/mxbai-embed-large-v1\" # for Docling, obtaining from HugginFace\n",
    "# Ollama equivalent: \"mxbai-embed-large:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e6045-cad0-4aa2-825d-e37c49e07adc",
   "metadata": {},
   "source": [
    "## Select Documents to Parse\n",
    "\n",
    "Create the `docs` folder and upload documents to use for the RAG process.\n",
    "We can include one or more files (e.g., PDFs, DOCX, Markdown, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76402a2d-be18-4109-9a6c-6852c37d2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_input=\",\".join(f\"docs/{f}\" for f in os.listdir('./docs'))\n",
    "print(f\"File list: {file_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf0876-f0db-4e47-b0bc-292b083d4eef",
   "metadata": {},
   "source": [
    "## Parse and Index Documents\n",
    "\n",
    "This step:\n",
    "- Parses each file asynchronously and extracts document elements\n",
    "- Indexes the text content into ChromaDB for fast semantic search\n",
    "\n",
    "Once this is done, we're ready to run queries against the indexed documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b919a30-ff38-49ca-8bd3-0b62ea264ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding caching to speed up demo\n",
    "\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import hashlib\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "PARSED_DIR = \"parsed\"\n",
    "os.makedirs(PARSED_DIR, exist_ok=True)\n",
    "\n",
    "def sha256_file(path: str, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "async def parse_and_cache(file_path: str):\n",
    "    \"\"\"Parse a file (async), cache results to .pkl if not already cached.\"\"\"\n",
    "    sha = await asyncio.to_thread(sha256_file, file_path)\n",
    "    cache_path = os.path.join(PARSED_DIR, f\"{sha}.pkl\")\n",
    "\n",
    "    # Load cached result if available\n",
    "    if os.path.exists(cache_path):\n",
    "        # Already processed — just load the cached result\n",
    "        print(f\"[cache] Loading parsed data for {file_path}\")\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            result = pickle.load(f)\n",
    "        return result\n",
    "\n",
    "    # Otherwise parse the document\n",
    "    print(f\"[parse] Processing {file_path} ...\")\n",
    "    parsed_result = await parse_document(file_path)  # existing async parser\n",
    "\n",
    "    # Save to pickle\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(parsed_result, f)\n",
    "    \n",
    "    return parsed_result\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "\n",
    "files_to_parse = [f.strip() for f in file_input.split(\",\") if f.strip()]\n",
    "all_elements = await asyncio.gather(*(parse_and_cache(f) for f in files_to_parse))\n",
    "\n",
    "for file_path, elements in zip(files_to_parse, all_elements):\n",
    "    print(f\"\\n Parsed {len(elements)} elements from '{file_path}'\")\n",
    "\n",
    "print(f\"time taken to parse: {time.time() - st:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231ab78-f709-4db9-b99d-41f017a2e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "st= time.time()\n",
    "index = index_all_elements(files_to_parse, all_elements, embedding_model)\n",
    "print(f\"time taken to index:{time.time()-st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592befb-fe99-4c95-a3fe-b6cd7e29d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release GPU memory if possible\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ffee58-be02-430b-b548-1b71a1f56e7b",
   "metadata": {},
   "source": [
    "## Ask a Question\n",
    "\n",
    "Now it's time to query the indexed documents!\n",
    "\n",
    "- Enter a question.\n",
    "- The system will:\n",
    "  1. Search for relevant text chunks using semantic similarity.\n",
    "  2. Send the top results along with the question to a language model.\n",
    "  3. Return a grounded answer based only on the retrieved context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94fda4-db3c-4a99-95bd-4ba5b40bd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is exosphere? How does it work, is it used in some project currently\"\n",
    "\n",
    "# Searching embeddings\n",
    "results = index.search(query)\n",
    "context = \"\\n\".join([text for text, score in results])\n",
    "\n",
    "# Gemerating answer using found context\n",
    "answer = generate_answer(context, query, temperature, max_tokens, model)\n",
    "\n",
    "print(f\"ANSWER: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3854782-e367-4425-a198-985eebeee5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"discuss Bare Metal Management Using OpenStack Ironic\"\n",
    "\n",
    "# Searching embeddings\n",
    "results = index.search(query)\n",
    "context = \"\\n\".join([text for text, score in results])\n",
    "\n",
    "# Gemerating answer using found context\n",
    "answer = generate_answer(context, query, temperature, max_tokens, model)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e3871-c5e0-4a0b-b36d-798c954c50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-oss:20b\"\n",
    "\n",
    "query = \"discuss OpenStack virtualized environments\"\n",
    "\n",
    "# Searching embeddings\n",
    "results = index.search(query)\n",
    "context = \"\\n\".join([text for text, score in results])\n",
    "\n",
    "# Gemerating answer using found context\n",
    "answer = generate_answer(context, query, temperature, max_tokens, model)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d312fdc-2778-4d34-a1bb-3ee6b1bb7d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAG_Pipeline_myvenv)",
   "language": "python",
   "name": "rag_pipeline_myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
