{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Search Agent with Web Search (Perplexity's `sonar`) and Ollama\n",
    "\n",
    "Original [Infotrend's CoreAI](https://github.com/Infotrend-Inc/CoreAI) Demo project: https://github.com/Infotrend-Inc/CoreAI-DemoProjects/tree/main/Agent\n",
    "**(modified to use Ollama and Perplexity's OpenAI API and only using the code's original \"Hybrid\" mode).**\n",
    "We recommend end users study the orignal code to see additional features.\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook demonstrates a comprehensive AI agent system that provides users with full control over model selection and processing approaches. \n",
    "The agent uses Perplexity's `sonar` model for web-enhanced responses and various local LLM models (through Ollama) for general processing.\n",
    "\n",
    "To enhance the functionality of the CoreAI  environment, we need to install some libraries not pre-installed but required for this notebook. \n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "To support features of this notebook with CoreAI, we need to install some libraries that are not pre-installed but are required. \n",
    "\n",
    "**Create and Activate the Virtual Environment:**\n",
    "\n",
    "Open a terminal within the Jupyter notebook (`File -> New -> Terminal`).\n",
    "Navigate to this project's folder; where we want to set up the environment (where this notebook is located) and run:\n",
    "\n",
    "```bash\n",
    "export PROJECT_NAME=\"SearchAgent\"\n",
    "export PIP_CACHE_DIR=`pwd`/.cache/pip\n",
    "mkdir -p $PIP_CACHE_DIR\n",
    "python -m venv --system-site-packages myvenv\n",
    "source myvenv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=${PROJECT_NAME}-myvenv --display-name=\"Python (${PROJECT_NAME}-myvenv)\"\n",
    "echo \"\"; echo \"Before continuing load the created Python kernel: Python (${PROJECT_NAME}-myvenv)\"\n",
    "```\n",
    "\n",
    "This will create a local virtual environment to contain installed files to the mounted `/iti` folder (and not modify the container's files).\n",
    "\n",
    "**Load the Python kernel described above before running the cell below** (it might take a few seconds for the kernel to appear in the list of kernels).\n",
    "\n",
    "Install the required Libraries (from `requirements.txt`).\n",
    "The rest of this notebook relies on the proper kernel to be loaded and environment variables to be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def set_env_with_cache_dir(env_var_name: str, subdir: str):\n",
    "    base_cache = os.path.join(os.getcwd(), \".cache\")\n",
    "    full_path = os.path.join(base_cache, subdir)\n",
    "    os.environ[env_var_name] = full_path\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "set_env_with_cache_dir(\"PIP_CACHE_DIR\", \"pip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "By default, the notebook will hide dot files (i.e. `.env`).\n",
    "Edit the `env.example` file included in this notebook to reflect used API keys and rename the file `.env` from a \"Terminal\".\n",
    "\n",
    "**Security Note**: Never commit API keys to version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment configuration file\n",
    "# This sets up the basic structure for API credentials\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "if 'PERPLEXITY_API_KEY' not in os.environ:\n",
    "    print(\"Missing PERPLEXITY_API_KEY -- FIX BEFORE CONTINUING\")\n",
    "print(\"✓ .env loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Imports all necessary Python libraries and modules required for the AI agent functionality. The imports include web requests handling, type definitions, environment variable management, datetime utilities, JSON processing, and the core LiteLLM library for model interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import requests\n",
    "from typing import List, Dict, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load and validate environment variables\n",
    "load_dotenv()\n",
    "PERPLEXITY_API_KEY = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "\n",
    "if not PERPLEXITY_API_KEY:\n",
    "    raise ValueError(\"Set PERPLEXITY_API_KEY in the .env file.\")\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama' # required but ignored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserChoiceAgent:\n",
    "    \"\"\"\n",
    "    A comprehensive AI agent that provides users with full control over model selection\n",
    "    and processing approaches. Supports both web-enabled Sonar models and regular LLMs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the agent with model discovery and user preference setup.\"\"\"\n",
    "        # Initialize model storage\n",
    "        self.available_models = []\n",
    "        self.sonar_models = []\n",
    "        self.regular_models = []\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Default user preferences\n",
    "        self.user_preferences = {\n",
    "            \"default_sonar_model\": \"sonar\",\n",
    "            \"default_llm_model\": \"llama3.1:8b\",\n",
    "            \"preferred_approach\": \"hybrid\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Response Generation Methods\n",
    "Implements the core response generation functionality for individual model interactions. The method handles the communication with LiteLLM proxy, manages API calls, processes responses, and provides comprehensive error handling for robust model interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(self, prompt: str, model: str) -> Dict:\n",
    "    \"\"\"Generate response using specified model.\"\"\"\n",
    "    try:\n",
    "        print(f\" Using {model}...\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"model\": model,\n",
    "            \"type\": \"llm_response\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error with {model}: {str(e)}\"}\n",
    "\n",
    "# Add method to UserChoiceAgent class\n",
    "UserChoiceAgent.generate_response = generate_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonar Search Methods\n",
    "Specializes in web-enabled search functionality using Perplexity's Sonar models. The implementation provides access to current web information, real-time data, and up-to-date content by leveraging Sonar's web search capabilities for enhanced response accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonar_search(self, query: str, sonar_model: str = None) -> Dict:\n",
    "    \"\"\"Use Sonar model for web-enabled search and response generation.\"\"\"\n",
    "\n",
    "    perplexity_client = OpenAI(api_key=PERPLEXITY_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "    \n",
    "    try:\n",
    "        print(f\" Using {sonar_model} for web search...\")\n",
    "        \n",
    "        response = perplexity_client.chat.completions.create(\n",
    "            model=sonar_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": query}],\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Sonar search failed: {str(e)}\"}\n",
    "\n",
    "    response_dict = {}\n",
    "    # Convert response to dict using model_dump() for Pydantic models\n",
    "    try:\n",
    "        response_dict = response.model_dump()\n",
    "    except AttributeError:\n",
    "        # Fallback for objects that don't support model_dump\n",
    "        response_dict = vars(response)\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "\n",
    "    # Add citations if the key is present in the response, irrelevant of the model provider\n",
    "    citations_text = \"\"\n",
    "    if 'citations' in response_dict:\n",
    "        citations_text += \"\\n\\nCitations:\\n\"\n",
    "        for i in range(len(response_dict['citations'])):\n",
    "            citations_text += f\"\\n[{i+1}] {response_dict['citations'][i]}\\n\"\n",
    "    response_text += citations_text\n",
    "\n",
    "    return {\n",
    "        \"content\": response_text,\n",
    "        \"model\": sonar_model,\n",
    "        \"type\": \"sonar_search\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Add method to UserChoiceAgent class\n",
    "UserChoiceAgent.sonar_search = sonar_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Processing Methods\n",
    "Implements the hybrid approach that combines web search with LLM processing for comprehensive responses. The method first gathers current information using Sonar models, then enhances and structures the content using regular LLMs for optimal readability and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_approach(self, query: str, sonar_model: str = None, llm_model: str = None) -> Dict:\n",
    "    \"\"\"Combine Sonar web search with LLM processing for comprehensive responses.\"\"\"\n",
    "    sonar_model = sonar_model or self.user_preferences[\"default_sonar_model\"]\n",
    "    llm_model = llm_model or self.user_preferences[\"default_llm_model\"]\n",
    "    \n",
    "    if not sonar_model or not llm_model:\n",
    "        return {\"error\": \"Both Sonar and LLM models required for hybrid mode\"}\n",
    "    \n",
    "    try:\n",
    "        print(f\" Hybrid Mode: {sonar_model} + {llm_model}\")\n",
    "        \n",
    "        # Step 1: Get web-grounded information from Sonar\n",
    "        print(\"** Step 1: Get web-grounded information from Sonar\")\n",
    "        sonar_result = self.sonar_search(query, sonar_model)\n",
    "        if \"error\" in sonar_result:\n",
    "            return sonar_result\n",
    "        \n",
    "        # Step 2: Use LLM to process and enhance the information\n",
    "        print(\"** Step 2: Use LLM to process and enhance the information\")\n",
    "        enhancement_prompt = f\"\"\"\n",
    "Based on the following web-grounded information, provide a comprehensive and well-structured answer to the query: \"{query}\"\n",
    "\n",
    "Web-grounded information:\n",
    "{sonar_result['content']}\n",
    "\n",
    "Please:\n",
    "When explaining, start with a one-paragraph “Executive Summary” section.\n",
    "Then add a “Core Concepts” section to explain in simple terms with related ideas on how it works. \n",
    "Follow this with a more detailed “Further Information” section, providing a longer description of content from the “Core Concepts” section.\n",
    "Add a “Related Concepts” section to include important things to know about a subject, but also key limitations, and things to know first.\n",
    "\n",
    "1. Organize the information clearly\n",
    "2. Add relevant context or explanations where helpful\n",
    "3. Maintain accuracy to the source information\n",
    "4. Structure the response for better readability\n",
    "\"\"\"\n",
    "        print(f\"-----------------------------------PROMPT:\\n {enhancement_prompt}\")\n",
    "        llm_result = self.generate_response(enhancement_prompt, llm_model)\n",
    "        if \"error\" in llm_result:\n",
    "            return llm_result\n",
    "        \n",
    "        return {\n",
    "            \"content\": llm_result['content'],\n",
    "            \"sonar_content\": sonar_result['content'],\n",
    "            \"sonar_model\": sonar_model,\n",
    "            \"llm_model\": llm_model,\n",
    "            \"type\": \"hybrid_response\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Hybrid processing failed: {str(e)}\"}\n",
    "\n",
    "# Add method to UserChoiceAgent class\n",
    "UserChoiceAgent.hybrid_approach = hybrid_approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing Methods\n",
    "Contains the main query processing logic that routes user requests to appropriate processing approaches. The method supports automatic approach detection, manual selection, and switching between different processing modes based on query characteristics and user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(self, query: str, options: Dict = None) -> Dict:\n",
    "    \"\"\"Process query with full user choice options.\"\"\"\n",
    "    if not options:\n",
    "        options = {}\n",
    "    \n",
    "    # Use user preferences as defaults\n",
    "    approach = options.get(\"approach\", self.user_preferences[\"preferred_approach\"])\n",
    "    sonar_model = options.get(\"sonar_model\", self.user_preferences[\"default_sonar_model\"])\n",
    "    llm_model = options.get(\"llm_model\", self.user_preferences[\"default_llm_model\"])\n",
    "    \n",
    "    # Auto-detect approach if needed\n",
    "    if approach == \"auto\":\n",
    "        search_keywords = ['latest', 'current', 'recent', 'news', 'today', 'update', 'what is happening']\n",
    "        if any(word in query.lower() for word in search_keywords):\n",
    "            approach = \"sonar_only\"\n",
    "        else:\n",
    "            approach = \"hybrid\"\n",
    "    \n",
    "    # Execute based on approach\n",
    "    if approach == \"sonar_only\":\n",
    "        result = self.sonar_search(query, sonar_model)\n",
    "    elif approach == \"llm_only\":\n",
    "        result = self.generate_response(query, llm_model)\n",
    "    elif approach == \"hybrid\":\n",
    "        result = self.hybrid_approach(query, sonar_model, llm_model)\n",
    "    else:\n",
    "        result = {\"error\": f\"Unknown approach: {approach}\"}\n",
    "    \n",
    "    # Add to conversation history\n",
    "    self.conversation_history.append({\n",
    "        \"query\": query,\n",
    "        \"result\": result,\n",
    "        \"options\": options,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Add method to UserChoiceAgent class\n",
    "UserChoiceAgent.process_query = process_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Methods\n",
    "Provides essential utility functions for preference management and model information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_preferences(self, **kwargs):\n",
    "    \"\"\"Update user preferences programmatically.\"\"\"\n",
    "    for key, value in kwargs.items():\n",
    "        if key in self.user_preferences:\n",
    "            self.user_preferences[key] = value\n",
    "            print(f\"✓ Updated {key} to {value}\")\n",
    "\n",
    "def list_models(self) -> Dict[str, List[str]]:\n",
    "    \"\"\"Return categorized list of available models and current preferences.\"\"\"\n",
    "    return {\n",
    "        \"sonar_models\": self.sonar_models,\n",
    "        \"regular_models\": self.regular_models,\n",
    "        \"all_models\": self.available_models,\n",
    "        \"user_preferences\": self.user_preferences\n",
    "    }\n",
    "\n",
    "# Add methods to UserChoiceAgent class\n",
    "UserChoiceAgent.update_preferences = update_preferences\n",
    "UserChoiceAgent.list_models = list_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent with user choice\n",
    "agent = UserChoiceAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Perform a websearch on different presentation at the OpenInfra 2025 Europe and discuss topics relevant to Artifical Intelligence and Scientific research\" \n",
    "result = agent.process_query(user_input)\n",
    "print(f\"\\n\\n\\n ---------------------------------Response:\\n{result.get('content', result.get('error', 'No response'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.user_preferences = {\n",
    "    \"default_sonar_model\": \"sonar\",\n",
    "    \"default_llm_model\": \"gpt-oss:20b\",\n",
    "    \"preferred_approach\": \"hybrid\"\n",
    "}\n",
    "\n",
    "user_input = \"What are the OpenInfra 2025 Europe presenations listed on LinkedIn. Reference and discuss topics relevant to Artifical Intelligence and Scientific research\" \n",
    "result = agent.process_query(user_input)\n",
    "print(f\"\\n\\n\\n ---------------------------------Response:\\n{result.get('content', result.get('error', 'No response'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Ollama\n",
    "\n",
    "The above code was designed during the summer 2025.\n",
    "At the end of September 2025, Ollama added a \"Web search\" capability to their API: https://ollama.com/blog/web-search\n",
    "\n",
    "The following makes uses of it without the various added features of the original Search Agent code: we will use `tools` to perform a query and use a local model to interpret the retrieved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment configuration file\n",
    "# This sets up the basic structure for API credentials\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "if 'OLLAMA_API_KEY' not in os.environ:\n",
    "    print(\"Missing OLLAMA_API_KEY -- FIX BEFORE CONTINUING\")\n",
    "print(\"✓ .env loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat, web_fetch, web_search\n",
    "\n",
    "available_tools = {'web_search': web_search, 'web_fetch': web_fetch}\n",
    "\n",
    "messages = [{'role': 'user', 'content': \"In LLM, what is an Agent, what are tool uses\"}]\n",
    "\n",
    "while True:\n",
    "  response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=messages,\n",
    "    tools=[web_search, web_fetch],\n",
    "    think=True\n",
    "    )\n",
    "  if response.message.thinking:\n",
    "    print('Thinking: ', response.message.thinking)\n",
    "  if response.message.content:\n",
    "    print('Content: ', response.message.content)\n",
    "  messages.append(response.message)\n",
    "  if response.message.tool_calls:\n",
    "    print('Tool calls: ', response.message.tool_calls)\n",
    "    for tool_call in response.message.tool_calls:\n",
    "      function_to_call = available_tools.get(tool_call.function.name)\n",
    "      if function_to_call:\n",
    "        args = tool_call.function.arguments\n",
    "        result = function_to_call(**args)\n",
    "        print('Result: ', str(result)[:200]+'...')\n",
    "        # Result is truncated for limited context lengths\n",
    "        messages.append({'role': 'tool', 'content': str(result)[:2000 * 4], 'tool_name': tool_call.function.name})\n",
    "      else:\n",
    "        messages.append({'role': 'tool', 'content': f'Tool {tool_call.function.name} not found', 'tool_name': tool_call.function.name})\n",
    "  else:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SearchAgent-myvenv)",
   "language": "python",
   "name": "searchagent-myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
