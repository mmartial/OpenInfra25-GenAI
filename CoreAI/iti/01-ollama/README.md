# Ollama

This demo shows how to use [Ollama](https://github.com/ollama/ollama) to run LLMs locally.

Steps demonstrated:

- [using `curl`](https://github.com/ollama/ollama/blob/main/docs/api.md) to pull LLM models and to run them locally
- using [Python's OpenAI API](https://pypi.org/project/openai/) to query models using Ollama as a backend
- using [Python's Ollama's API](https://pypi.org/project/ollama/) to query local models
