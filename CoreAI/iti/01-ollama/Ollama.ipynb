{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c9a4ee-cb43-4e89-aec0-d7310d43fea8",
   "metadata": {},
   "source": [
    "# Ollama access\n",
    "\n",
    "Our `compose.yaml` has started Ollama on the Tailscale network (`tailnet`) and make it accessible directly on `localhost` (ie both the notebook and Ollama are exposed over Tailscale) at `http://localhost:11434` from within the running container (although the service is on a different docker IP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f3f32-8381-41d4-8b9d-fb8c75b1d7dc",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "To support features of this notebook with CoreAI, we need to install some libraries that are not pre-installed but are required for this notebook. \n",
    "\n",
    "**Create and Activate the Virtual Environment:**\n",
    "\n",
    "Open your terminal or command prompt within the Jupyter notebook. Navigate via `File -> New -> Terminal`.\n",
    "Type `bash` to access a shell compatible with the following commands.\n",
    "Navigate to the project directory where you want to set up the environment (where this notebook is located):\n",
    "\n",
    "```bash\n",
    "export PROJECT_NAME=\"Ollama\"\n",
    "export PIP_CACHE_DIR=`pwd`/.cache/pip\n",
    "mkdir -p $PIP_CACHE_DIR\n",
    "python -m venv --system-site-packages myvenv\n",
    "source myvenv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=${PROJECT_NAME}_myvenv --display-name=\"Python (${PROJECT_NAME}_myvenv)\"\n",
    "echo \"\"; echo \"Before continuing load the created Python kernel: Python (${PROJECT_NAME}_myvenv)\"\n",
    "```\n",
    "\n",
    "Load the Python kernel described above before running the cell below (it might take a few seconds for the kernel to appear in the list of kernels).\n",
    "\n",
    "**AFTER the kernel is loaded, install the required Libraries (from `requirements.txt`)**\n",
    "\n",
    "The rest of this notebook relies on the proper kernel to be loaded and environment variables to be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e22f5b-01e7-4c6f-86e5-3d7d0d8d9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344b5ea-0744-4d1e-ae46-cf5abbf60308",
   "metadata": {},
   "source": [
    "[Infotrend's CoreAI](https://github.com/Infotrend-Inc/CoreAI) is an Ubuntu 24.04 based Docker container with PyTorch, OpenCV (GPU build) and CUDA.\n",
    "\n",
    "Being Ubuntu based, we can install components using `apt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbb226-be3d-4749-ad78-c816cd26db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt update && sudo apt install -y jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406574c-d1e1-4b06-9ab8-8a59ff692321",
   "metadata": {},
   "source": [
    "## Ollama pulling a model using `docker exec`\n",
    "\n",
    "It is possible to directly ask the container to perform operations using the `ollama` CLI.\n",
    "\n",
    "```bash\n",
    "docker exec -it oi25-coreai-ollama ollama pull llama3.1:8b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf6555-959f-4af0-885b-c0995a851113",
   "metadata": {},
   "source": [
    "## Ollama access from a shell: using `curl`\n",
    "\n",
    "Details on how to use the API using is available at https://github.com/ollama/ollama/blob/main/docs/api.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e81e9-4b87-4c91-8e11-8438e5a5512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull a model\n",
    "! curl http://localhost:11434/api/pull -d '{ \"model\": \"llama3.1:8b\" }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0b2f0-6241-485f-9f43-bb3c190c6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl http://localhost:11434/api/pull -d '{ \"model\": \"gpt-oss:20b\" }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8edee-0c82-4f62-a4af-9a3b4f8deded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check list of downloaded model(s)\n",
    "! curl -X GET http://localhost:11434/api/tags | jq '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2135b40-58b9-4eb3-ab22-434db38fb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Model specific information\n",
    "! curl http://localhost:11434/api/show -d '{ \"model\": \"llama3.1:8b\" }' | jq '.model_info'\n",
    "! curl http://localhost:11434/api/show -d '{ \"model\": \"gpt-oss:20b\" }' | jq '.model_info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69061f01-fbaf-46d1-9719-2afb398027a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## Asking a question of the model\n",
    "# - we will disable streaming to get a sentence (versus word per word)\n",
    "# - we will set the `seed` and use a `temperature` of `0`, ie when asking the same question of the same model we will get the same answer\n",
    "\n",
    "\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.1:8b\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is OpenStack\"\n",
    "    }\n",
    "  ],\n",
    "  \"options\": {\n",
    "    \"seed\": 101,\n",
    "    \"temperature\": 0\n",
    "  },\n",
    "  \"stream\": false\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9851cfa-89fd-47f0-b736-9b91452478a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## Asking a question of the model\n",
    "# - we will disable streaming to get a sentence (versus word per word)\n",
    "# - we will set the `seed` and use a `temperature` of `0`, ie when asking the same question of the same model we will get the same answer\n",
    "\n",
    "\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"gpt-oss:20b\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is OpenStack\"\n",
    "    }\n",
    "  ],\n",
    "  \"options\": {\n",
    "    \"seed\": 101,\n",
    "    \"temperature\": 0\n",
    "  },\n",
    "  \"stream\": false\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c43f86-89fd-4f38-b0c1-7c8c04b981e0",
   "metadata": {},
   "source": [
    "# Ollama access with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a494c4-2050-4710-a54b-aa9e5f684661",
   "metadata": {},
   "source": [
    "### OpenAI API\n",
    "\n",
    "We are using Ollama's OpenAI compatiblity to access the installed model. For more details, see https://github.com/ollama/ollama/blob/main/docs/openai.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5b659-f5ef-4508-a247-bdb17ae91551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama' # required but ignored\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'What is the OpenInfra foundation',\n",
    "        }\n",
    "    ],\n",
    "    model='llama3.1:8b'\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3dede4-2ab5-4745-bc54-f45813fa18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'What is the OpenInfra foundation',\n",
    "        }\n",
    "    ],\n",
    "    model='gpt-oss:20b'\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfef089-e127-4f58-99ab-95e25b46a214",
   "metadata": {},
   "source": [
    "### Ollama API\n",
    "\n",
    "https://github.com/ollama/ollama-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c736f05-c390-4034-981c-4e508a81e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the OpenStack Scientific SIG?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed90473-f5df-4261-9c7d-a6b3a09813ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Ollama_myvenv)",
   "language": "python",
   "name": "ollama_myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
