{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original [Infotrend's CoreAI](https://github.com/Infotrend-Inc/CoreAI) Demo project: https://github.com/Infotrend-Inc/CoreAI-DemoProjects/tree/main/Gemma3\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook uses Google’s Gemma 3 open models locally (using its instruction-tuned checkpoint: `gemma-3-4b-it`) to show Large Language Model (LLM) and Vision Language Model (VLM) capabilities. without using a service such as Ollama to `serve` the model.\n",
    "\n",
    "High-level steps:\n",
    "\n",
    "1. Install/load runtime packages.  ￼\n",
    "2. Initialize tokenizer and model\n",
    "3. Prompting (text): a small chat message (system/user) to answer a question.\n",
    "4. Multimodal (image): perform image understanding on a provided image.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "To support features of this notebook with CoreAI, we need to install some libraries that are not pre-installed but are required. \n",
    "\n",
    "**Create and Activate the Virtual Environment:**\n",
    "\n",
    "Open a terminal within the Jupyter notebook (`File -> New -> Terminal`).\n",
    "Navigate to this project's folder; where we want to set up the environment (where this notebook is located) and run:\n",
    "\n",
    "\n",
    "```bash\n",
    "export PROJECT_NAME=\"Gemma3\"\n",
    "export PIP_CACHE_DIR=`pwd`/.cache/pip\n",
    "mkdir -p $PIP_CACHE_DIR\n",
    "python -m venv --system-site-packages myvenv\n",
    "source myvenv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=${PROJECT_NAME}-myvenv --display-name=\"Python (${PROJECT_NAME}-myvenv)\"\n",
    "echo \"\"; echo \"Before continuing load the created Python kernel: Python (${PROJECT_NAME}-myvenv)\"\n",
    "```\n",
    "\n",
    "This will create a local virtual environment to contain installed files to the mounted `/iti` folder (and not modify the container's files).\n",
    "\n",
    "**Load the Python kernel described above before running the cell below** (it might take a few seconds for the kernel to appear in the list of kernels).\n",
    "\n",
    "Install the required Libraries (from `requirements.txt`).\n",
    "The rest of this notebook relies on the proper kernel to be loaded and environment variables to be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!. ./myvenv/bin/activate && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `accelerate` to the PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "os.environ['PATH'] =  os.path.join(pwd, 'myvenv/bin') + os.pathsep + os.environ['PATH']\n",
    "\n",
    "! echo $PATH\n",
    "! which accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure `HF_HOME` is set BEFORE `transformers` is loaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('HF_HOME', exist_ok=True)\n",
    "os.environ['HF_HOME'] = 'HF_HOME'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the notebook will hide dot files (i.e. `.env`).\n",
    "Edit the `env.example` file included in this notebook to reflect the required API keys and rename the file `.env` from a \"Terminal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment configuration file\n",
    "# This sets up the basic structure for API credentials\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "if 'HF_TOKEN' not in os.environ:\n",
    "    printf(\"No HF_TOKEN set, will not be able to download the model\")\n",
    "    exit(1)\n",
    "\n",
    "hf_token=os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM: Large Language Model queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM: Obtain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is OpenStack? What is its landscape? What is the scientific SIG?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"you are a helpful assistant\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": f\"{question}\"}]\n",
    "    },\n",
    "]\n",
    "user_message = messages[1][\"content\"][0][\"text\"]\n",
    "\n",
    "output = pipe(text_inputs=user_message, max_new_tokens=4000)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When done using the LLM: free up GPU memory (when running on 24GB VRAM)\n",
    "\n",
    "Note: we are using the same variable for text and image (`pipe`) this code can be used when switching from one method to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up the model from memory before testing the VLM (Garbage Collection)\n",
    "import gc\n",
    "\n",
    "# check memory\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "del pipe\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# check memory again\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM: Image Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM: Obtain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM: Exract information from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt `img_file` to the file to be analyzed.\n",
    "\n",
    "Modify the prompt to answer different types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL \n",
    "import base64\n",
    "import io\n",
    "from IPython.display import display\n",
    "\n",
    "img_file = \"data/openstack-map-v20250401.png\"\n",
    "\n",
    "\n",
    "def base64_image(img_file):\n",
    "    img_type = \"png\"\n",
    "    img_b64 = None\n",
    "    img_str = None\n",
    "    img_bytes = io.BytesIO()\n",
    "    with PIL.Image.open(img_file) as image:\n",
    "        display(image)\n",
    "        image.save(img_bytes, format=img_type)\n",
    "        img_b64 = base64.b64encode(img_bytes.getvalue()).decode('utf-8')\n",
    "    if img_b64 is not None:\n",
    "        img_str = f\"data:image/{img_type};base64,{img_b64}\"\n",
    "    \n",
    "    if img_str is None:\n",
    "        print(\"No valid image data\")\n",
    "        exit(1)\n",
    "\n",
    "    return img_str\n",
    "\n",
    "img_str = base64_image(img_file)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image_url\", \"image_url\": { \"url\": img_str } },\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image. what is the field of expertise needed, explain the idea behind the meaning of the image?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=1000)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Gemma3-myvenv)",
   "language": "python",
   "name": "gemma3-myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
