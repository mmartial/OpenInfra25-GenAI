### ComfyUI

[https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

<div style="font-size: 0.7em;">

- ğŸ§© Modular & Visual â€” Graph-based interface for building AI image generation workflows (without coding).
- ğŸ”— Node-Driven Architecture â€” Everything is built from nodes â€” each performing a specific task in the pipeline.
- ğŸ§  Flexible Workflows â€” Users connect visual nodes (prompts, samplers, models, processors) to create complex pipelines.
- ğŸ§° Customizable â€” Supports `custom nodes`, enabling fine-tuned control at every stage.
- ğŸª„ Pipeline Flow â€” Prompt â Encode â Model â Noise â Sampler â Decode â Output.

</div>

--

### ComfyUI-Nvidia-Docker

[https://github.com/mmartial/comfyui-nvidia-docker](https://github.com/mmartial/comfyui-nvidia-docker)

<div style="font-size: 0.7em;">

- ğŸ³ ComfyUI in a container with NVIDIA GPU support, bundling CUDA, drivers, and all dependencies.
- ğŸ§‘â€ğŸ’» Clean File Permissions â€” Supports `comfy` user UID/GID mapping to match host users and avoid permission issues on shared volumes.
- ğŸ§­ Easy Management â€” Integrates ComfyUI-Manager for smooth updates and node handling.
- ğŸ§© Flexible Configuration â€” Enables `user scripts`, custom launch args, and isolated data through separate `run` & `basedir` folders.
- ğŸ” Security Controls â€” Provides configurable ComfyUI `security levels` to match your environment.

</div>

--

### Stable Diffusion terminology

<div style="font-size: 0.5em;">

| Component | Role | Analogy |
| --- | --- | --- |
| ğŸ§  Model | Core generator turning text into images | ğŸ¨ Painter |
| ğŸ“– CLIP | Encodes text into concepts the model understands | ğŸ—£ï¸ Translator |
| ğŸ§© LoRA | Adds new styles or knowledge to the base model | ğŸ§° Custom brush |
| ğŸ”„ Sampler | Algorithm that refines noise into the image | ğŸ”„ Painting technique |
| ğŸŒŒ Latent | Compressed internal image representation | ğŸ“ Blueprint |
| ğŸ–¼ï¸ VAE | Converts latent image to actual pixels | ğŸ–¨ï¸ Printer |

</div>

<div style="font-size: 0.5em;">

[https://en.wikipedia.org/wiki/Stable_Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion)

</div>

--

### Pre-requisite

<div style="font-size: 0.5em;">

[https://github.com/mmartial/OpenInfra25-GenAI/tree/main/ComfyUI](https://github.com/mmartial/OpenInfra25-GenAI/tree/main/ComfyUI)

</div>

- [Docker](https://docs.docker.com/get-docker/)
    - [Docker Compose](https://docs.docker.com/compose/)
- a [Tailscale](https://tailscale.com/) account
    - containers will be used in a Tailscale network (`tailnet`) and have no exposed ports
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
    - an NVIDIA GPU with at least 24GB of VRAM
- a [ComfyUI](https://github.com/comfyanonymous/ComfyUI) environment
    - container: [mmartial/comfyui-nvidia-docker](https://github.com/mmartial/comfyui-nvidia-docker)

--

### Getting started

<div style="font-size: 0.5em;">

[https://github.com/mmartial/OpenInfra25-GenAI/blob/main/ComfyUI/compose.yaml.example](https://github.com/mmartial/OpenInfra25-GenAI/blob/main/ComfyUI/compose.yaml.example)

</div>

<div style="font-size: 0.8em;">

```yaml
services:
  oi25-comfyui-nvidia:
    image: mmartial/comfyui-nvidia-docker:latest
    volumes:
      - ./run:/comfy/mnt
      - ./basedir:/basedir
    environment:
      - WANTED_UID=<WANTED_UID> # TODO: replace this to your user ID `id -u`
      - WANTED_GID=<WANTED_GID> # TODO: replace this to your group ID `id -g`
      - BASE_DIRECTORY=/basedir
      - SECURITY_LEVEL=normal

  tailscale-oi25-comfyui:
    image: tailscale/tailscale:latest
    hostname: tailscale-oi25-comfyui
    environment:
      - TS_AUTHKEY=tskey-auth- # TODO: Add your TS_AUTHKEY here
```

</div>

No ports exposed: access through `tailscale-oi25-comfyui`'s Tailscale IP

--

### Flux.1 "tok man" Low Rank Adaptation (LoRA) 

<div style="font-size: 0.5em;">

[https://www.gkr.one/blg-20240818-flux-lora-training](https://www.gkr.one/blg-20240818-flux-lora-training)

</div>

FLUX.1 LoRA to generate new outputs reflecting the training subject

<div style="font-size: 0.7em;">

- ğŸª„ Training Details â€” LoRA trained for 4,000 steps using 25 input images.
- âš¡ Hardware Performance â€” On an NVIDIA RTX 4090, training completed in â‰ˆ140 minutes.
- ğŸ’¾ Model Output â€” Final LoRA weight file `flux_dev-tok.safetensors` is ~200 MB.

</div>
<hr>

<div style="font-size: 0.8em;">

- FLUX.1-dev [https://huggingface.co/black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)
- FLUX.1 LoRA training [https://www.gkr.one/blg-20240818-flux-lora-training](https://www.gkr.one/blg-20240818-flux-lora-training)
- ai-toolkit [https://github.com/ostris/ai-toolkit](https://github.com/ostris/ai-toolkit)

</div>

--

![Tokan result](https://github.com/mmartial/OpenInfra25-GenAI/blob/main/ComfyUI/01-LoRA_tokman/tokman_ex01_02.jpg?raw=true)

--

### Flux Kontext

<div style="font-size: 0.5em;">

[https://github.com/mmartial/OpenInfra25-GenAI/tree/main/ComfyUI/02-Flux_Kontext](https://github.com/mmartial/OpenInfra25-GenAI/tree/main/ComfyUI/02-Flux_Kontext)

</div>

<div style="font-size: 0.7em;">

- ğŸª„ In-Context Image Editing + Generation â€” Provide an image + text instruction, and FLUX Kontext interprets the scene and applies the requested edit.
- ğŸ§­ Local & Global Edits â€” Make targeted changes (e.g. change hair color, remove or add objects) or transform entire scenes, styles, or layouts.
- ğŸ§ Character/Object Consistency â€” Successive edits preserve identity, style & composition, minimizing visual drift across multiple editing steps.

</div>
<hr>

<div style="font-size: 0.8em;">

- FLUX.1 Kontext [https://bfl.ai/models/flux-kontext](https://bfl.ai/models/flux-kontext)
- ComfyUI Flux Kontext [https://docs.comfy.org/tutorials/flux/flux-1-kontext-dev](https://docs.comfy.org/tutorials/flux/flux-1-kontext-dev)

</div>

--

<div style="font-size: 0.7em;">

1. text removal + re-color black and white source
2. background addition + outfit change
3. outfit change + add the OpenInfra logo on the outfit
4. style change

</div>

![Flux Kontext result](https://github.com/mmartial/OpenInfra25-GenAI/blob/main/ComfyUI/02-Flux_Kontext/fluxkontext-ex01_04.jpg?raw=true)

--

### WAN 2.2 Animate

<div style="font-size: 0.5em;">

[https://github.com/mmartial/OpenInfra25-GenAI/tree/main/ComfyUI/03-WAN2.2_Animate](https://github.com/mmartial/OpenInfra25-GenAI/tree/main/ComfyUI/03-WAN2.2_Animate)

</div>

<div style="font-size: 0.7em;">

- ğŸ¤– Core Tools â€” WAN 2.2 Animate + Kijaiâ€™s `custom node` `ComfyUI-WanAnimatePreprocess` (+ embedded workflow).
- ğŸ§ Preprocessing Pipeline â€” Detects people in frames, estimates body / hand / face keypoints, extracts aligned face crops, and formats results for WanAnimate.
- ğŸ¬ Result â€” Transforms a single character image + reference video into a high-fidelity animated or replacement video.
- âš ï¸ GPU â€” Minimum 32 GB VRAM (even with `fp8` weights).

</div>
<hr>

<div style="font-size: 0.8em;">

- WAN 2.2 Animate [https://huggingface.co/Wan-AI/Wan2.2-Animate-14B](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B)
- Kijai's `ComfyUI-WanAnimatePreprocess` [https://github.com/kijai/ComfyUI-WanAnimatePreprocess](https://github.com/kijai/ComfyUI-WanAnimatePreprocess)
</div>

--

![WAN 2.2 Animate result](https://github.com/mmartial/OpenInfra25-GenAI/blob/main/ComfyUI/03-WAN2.2_Animate/wan2.2animate.jpg?raw=true)
